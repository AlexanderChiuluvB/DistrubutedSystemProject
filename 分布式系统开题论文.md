## 分布式系统课程项目——电商秒杀模拟

### 秒杀业务分析

- 一致性：库存量少，一般秒杀请求量远远大于库存数量，因此在大并发更新的时候要防止超卖现象发生。
- 高可用：系统能够抗住大量的流量同时保证不宕机。
- 高性能：秒杀涉及大量并发读和并发写，而且网络流量暴增，网络带宽压力会加大，因此要支持高并发访问。因此网络带宽将是性能提升的一个瓶颈。
- 业务流程简单：下购买订单->数据库库存减少->支付订单
- 读多写少：大量的HTTP请求都是读请求，写请求即更改数据库的请求只占少数，因此提高数据库的读取速度将是性能提升的一关键。

### 架构设计思路

所有措施的根本目的都是为了把请求尽量拦截在上游，尽可能减少对MYSQL数据库的访问。

- 限流

屏蔽掉大部分的用户流量，只允许少量用户进入后端服务器。

具体实现: Redis令牌桶的限流算法屏蔽大量流量。

- 削峰

秒杀请求在时间上高度集中于某一个时间点，瞬时流量容易冲垮系统。因此需要对瞬时流量进行削峰处理，缓冲瞬时流量，尽量让服务器对资源进行平缓处理

具体实现：大量下单请求先经过Kafka队列入队，然后再缓慢出队。

- 异步

把同步的下单请求改为异步，提高并发量，本质上也是削峰操作

- 充分利用缓存

创建订单的时候，每次都需要查询Redis判断库存是否足够。只有少部分成功的请求才会创建订单。

具体实现：可以把商品信息放在Redis缓存中，减少数据库查询。

- 负载均衡

利用Nginx等使用多个服务器并发处理请求，以减少单个服务器的压力.

### 高并发下的数据安全与解决方案

- 超卖现象

假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是1个，然后都通过了这一个余量判断，最终导致超卖，这是我们要极力避免的现象。

#### 解决方案分析

- 悲观锁

每个线程在尝试修改数据的时候，会给数据项上锁，以阻止其他线程修改。如果其他线程尝试访问上了锁的数据，就会一直等待。这种解决方案缺点是性能很慢。

- Mysql乐观锁

tbc

- 基于Redis的分布式锁

tbc

- 基于Zookeeper的分布式锁

主要利用了Zookeeper文件系统的Znode.上锁就是某个节点尝试创建临时的znode，创建成功了就相当于获取这个锁。

这个时候如果别的客户端来尝试创建锁就会失败，所以只能注册一个监听器来监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以重新加锁。

理论上，Redis是基于CAS，需要自己不断去尝试获取锁，比较消耗CPU性能。

但是Zk分布式锁如果获取不到锁，只需要注册一个监听器即可，不需要不断主动尝试获取锁，性能开销相对较小。

而且Zk分布式锁的语义更为清晰简单，省去了遍历上锁，计算超时时间的步骤。


### 代码架构&秒杀流程代码分析


#### 架构改进前：

![架构改进前](https://raw.githubusercontent.com/gongfukangEE/gongfukangEE.github.io/master/_pic/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%BC%93%E5%86%B2.png)

请求先经过Redis令牌桶算法过滤部分请求，然后用Redis来访问

解决超卖的关键：Mysql基于版本号的乐观锁

性能瓶颈：Kafka消费速度

缺点：

没有充分利用Redis,Redis的作用仅仅在于令牌桶过滤部分请求以及所有商品售完后用来挡住后续的秒杀请求，秒杀完成之前，
大量无效的请求最终会落到数据库上，而数据库查询会涉及到磁盘寻道和磁盘IO，性能十分慢。


#### 架构改进后：

解决超卖的关键：

性能瓶颈：

有待继续改进地方：(分布式锁大并发量下不靠谱)

解决方案：最后Mysql更新语句 + and count > 0;

### Redis

tbc..

### Kafka

#### Kafka简介

Kafka是一个分布式的基于Zookeeper协调的发布订阅系统。

主要应用场景: 日志收集系统与消息系统。

Kafka主要设计目标：

- 以O(1)时间复杂度提供消息持久化的能力，对TB级别的数据也能保证常数时间的访问性能。
- 高吞吐率。在廉价机器上也能做到单机支持100K/s速率的消息传输
- 支持消息分区，以及分布式消费。可以保证每个分区内部消息的顺序传输
- 同时支持离线数据处理与实时数据处理

#### 发布订阅消息系统

![img](https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190443404-1266011458.png)

图片ref:https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190443404-1266011458.png

Kafka是一种发布-订阅消息系统，所谓发布订阅系统，指的是发布者不直接把信息发给消费者，发布者把消息持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或者多个topic，消费者可以消费该topic中所有的数据，而且同一条数据可以被多个消费者消费。

发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。

#### Kafka在本项目中的作用

- 解耦

解耦指的是在项目启动的时候，难以预测未来的具体需求。那么消息系统可以在处理过程中插入一个隐含的，基于数据的接口，两边的处理过程都要实现这个接口。

具体在本项目中，解耦具体表现在订单访问redis判断预库存量以及最终访问数据库库存量中间。

- 削峰

削峰指有时候数据量会剧增，那么如果以处理这类峰值访问为标准来投入资源，会造成巨大的浪费。因此使用消息队列能够使得关键组件顶住突发的访问压力，使得系统不会因为突发的超负荷请求而完全崩溃。

具体在本项目中，削峰体现在大量的秒杀请求入队然后通过消息队列缓慢出队，以减少对数据库访问的压力。

- 可扩展性

消息队列解耦了消息处理过程，所以使得调节消息处理速度也变得更加方便。可以通过简单改变集群数，分区数就可以轻易地增大消息入队和处理的频率。

- 异步通信

异步通信指消费者有时候不想立刻处理生产者发来的消息，因此消息队列提供了异步处理机制。允许用户把一个消息放入队列，但是不立即处理。

具体在本项目中，体现在把整个下单访问数据库的过程异步化，每个请求的成功访问与否互不关联，提高了消息处理的效率。

#### Kafka基本概念

- Broker

kafka集群中包括多个服务器，单个服务器节点即broker. broker存储着topic的partition数据

- Topic

每一条发布到Kafka集群的消息都有一个类别，即Topic. 属于逻辑上的消息集合概念，实际物理上同一个Topic的多个分区数据可能是存储在不同broker上的，但是对使用者来说不必关心数据存于何处

- Partition

topic中的数据实际上会分布在一个或者多个partition上存储。每个分区中的数据使用多个segment文件存储。分区内的数据可以保证有序性。

- Producer

消息的发布者，角色把消息发布到Kafka的topic中，broker接收到生产者发送的消息后，broker会把消息**追加**到当前用于存储数据的segment中。

- Consumer

消费者负责从broker中读取topic数据，本项目中采用spring-kafka，在秒杀服务启动的时候会额外启动一个消费者监听线程，负责监听Kafka集群中的分区，消费数据。

#### Kafka在本项目中的部署与参数配置优化

本项目在五台服务器上共搭建5节点的Kafka Broker与ZooKeeper集群

| 服务器 | ip          | 配置         |
| ------ | ----------- | ------------ |
| admin  | 172.101.8.2 | 8cpu 16G内存 |
| node1  | 172.101.8.3 | 8cpu 16G内存 |
| node2  | 172.101.8.4 | 8cpu 16G内存 |
| node3  | 172.101.8.5 | 8cpu 16G内存 |
| node4  | 172.101.8.6 | 8cpu 16G内存 |



Kafka Broker配置优化：

```markdown
# 用于接受并处理网络请求的线程数，默认为3，实际上为Kafka内部轮询机制中负责读取请求的线程数。
# 本项目由于上下游并发请求量过大，因此为了尽可能减少io等待，配置线程数量为cpu核数+1
num.network.threads=9
```

```markdown
# 负责磁盘io操作的线程数，默认为8，因为Kafka生产和消费过程中都会伴随着
# 数据的落盘，为了提高入队和出队的速率，可以适当增加处理磁盘的io线程数
num.io.threads=16
```

```markdown
# 每隔1s就刷写一次磁盘
log.flush.interval.ms=1000
```

```Markdown
# 设定发送消息后不需要Broker端返回确认，虽然存在丢失数据的风险，但是由于本项目对数据完整性以及数据消费顺序没有要求，因此吞吐量能达到最大
acks: 0
```

### 性能瓶颈分析与解决方案

问题1：Kafka消费速度过慢及产生消费挤压问题

刚开始测试的时候，Kafka集群的分区数量为5，大概每一个节点负责维护1个分区。这就导致了进队速度远远大于出队速度，导致大量HTTP请求产生了积压。

架构改进前：

具体来说，我们采取的是基于版本号的乐观并发更新机制，意味着每个HTTP请求都会伴随一个版本号，每个HTTP请求生成的时候都会去Redis请求获得当前的版本号。只有在当前版本号与当前Mysql记录的版本号相等的HTTP请求才能成功触发库存的更新，更新成功后才会去更新Mysql和Redis版本号。

举个例子，试想如果生产速度远远高于消费速度，假设现在最新的版本号为1，那么在时刻t1产生的5000个请求都会在Redis请求当前的版本号，所以这5000个请求的版本号都为1. 那么第一个出队的HTTP的请求会成功触发Mysql库存更新，版本号更新为2。但是接下来的时间都是用来处理剩下的4999个请求，这4999个请求由于版本号为1，所以更新失败。所以系统大量时间浪费在处理无效的对Mysql访问的HTTP请求，导致新的版本号对应的请求无法及时访问Mysql，导致秒杀商品难以迅速售光。

架构改进后：

这时候因为Redis已经扛住了大量的无效请求，所以发送给Kafka的请求量其实并不大。因此对消费速度的要求其实并不高。

提升Kafka消费速度的解决方案：


- 增加分区数量，结合线程池进行多线程消费

分区数量从5个增加到100个，平均每个服务器节点handle20个分区。理论上分区数目越多，kafka的吞吐量会越大，处理消息的速度也会越快。因为分区越多，间接减少了单个分区的消息负载的压力，减少了单个分区消费积压发生的可能性。在增大分区后，生产者还是随机把秒杀请求分发到100个分区，而原来的Kafka监听单线程改写为使用线程池，开启100个监听线程，每个消费线程具体负责消费1个分区的HTTP请求。

- 消费者关闭auto commit自动提交偏移量选项

设置enable-auto-commit:false

分区偏移量的更新涉及到网络的IO，因此可以选择手动提交偏移量offset，以减少网络IO开销，减轻带宽压力，进而提高消费速度。

- 分批消费消息，增加单次拉取消息的大小

修改max-poll-records为10000，即单次消费数据最大条数为10000.

并且把逐条消费数据改为分批消费数据，每次网络IO能处理更多的消费量，同样减少了消费者的IO次数，提高了消费速度。

- 生产者采取消息序列化与压缩算法

首先把Json格式的HTTP请求序列化为字节流，减少要传递的消息大小。并且kafka在消息传递的时候内置支持了许多压缩算法，这里采用压缩性能比最高的gzip算法。从而能够减少通信的开销。

- 在Kafka属性配置文件中调参

详细介绍见Kafka在本项目中的部署与参数配置优化一节中的介绍，核心思想就是通过增加IO处理的线程数量，减少传输次数与传输的数据量量以减轻网络带宽的压力，以及牺牲数据的完整性有序性来换取尽可能高的吞吐量。

经过上述优化后，消费速度提升非常明显，能够较好地解决消费滞后和消费挤压问题，库存商品卖不完的问题也得到了解决。

经分析，网络带宽才是我们的整个项目的性能最大瓶颈。因为我们的秒杀服务一开始是在本地部署的，而中间件都是部署在学校的服务器，本地部署的秒杀服务把Restful请求传输到远程服务器上，这段时间开销比较大。后来当把秒杀服务部署到远程服务器后，秒杀服务与Kafka，Redis和Mysql同在一个局域网内，因此网络io速度也大大提高，经测试，秒杀完成速度提高了3-4倍。


问题2： 改进前架构没有充分利用Redis




### Nginx

tbc..

### Mysql

tbc..

### 测试方案

本项目使用Apache Jmeter来模拟大量的HTTP请求，

jmx配置：模拟生成1千万个http请求,产生http请求的速度制约于秒杀系统的性能。

架构改建前：

- Redis缓存+Kafka削峰+Mysql乐观锁更新方案：

  最高tps：秒杀过程中600/s,库存为0之后继续访问的请求tps1300/s

- Redis缓存+Kafka削峰+Mysql乐观锁更新+Nginx分发到3台服务器方案：

  最高tps:  秒杀过程中tps1000/s，库存为0之后继续访问的请求tps2500/s

- Redis缓存+Kafka削峰+Mysql乐观锁更新+Nginx分发到7台服务器方案

  最高tps:  秒杀过程中tps1000/s，库存为0之后继续访问的请求tps2500/s
  
  
架构改进后：

- Redis缓存+Kafka削峰+Redis分布式锁 秒杀服务单机部署：
  最高tps： 16000-18000/s
  
- Redis缓存+Kafka削峰+Zookeeper分布式锁 秒杀服务单机部署：
  最高tps： 16000-18000/s
  
  

### 结论

tbc...

### Ref

tbc..
